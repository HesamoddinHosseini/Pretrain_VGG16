# -*- coding: utf-8 -*-
"""Copy of Pretrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hZvgKJeYG452i7YovGiqHOPfZzCxEgIl
"""

# https://tensorlayer.readthedocs.io/en/latest/modules/prepro.html
# https://keras.io/api/models/model_saving_apis/
# http://ai.bu.edu/M3SDA/

import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras.layers as layers

log_dir = "logs/"
BATCH_SIZE = 32
BUFFER_SIZE = 1000
images_shape = [224, 224, 3]
images_size = (224, 224)
input_size = (224, 224, 3)
embedding_size = 64
class_count = 10
EPOCHS = 5

!wget http://csr.bu.edu/ftp/visda/2019/multi-source/real.zip

!unzip real.zip

from google.colab import drive
drive.mount('/content/drive')

!cp -r real/airplane drive/MyDrive/data/airplane

!cp -r real/apple drive/MyDrive/data/apple

!cp -r real/arm drive/MyDrive/data/arm
!cp -r real/bird drive/MyDrive/data/bird
!cp -r real/book drive/MyDrive/data/book
!cp -r real/belt drive/MyDrive/data/belt
!cp -r real/bicycle drive/MyDrive/data/bicycle
!cp -r real/brain drive/MyDrive/data/brain
!cp -r real/candle drive/MyDrive/data/candle
!cp -r real/car drive/MyDrive/data/car

data_dir = '/content/drive/MyDrive/data/'
train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.5,
                                                               subset="training", seed=123,
                                                               image_size=images_size, batch_size=BATCH_SIZE)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir, validation_split=0.5, subset="validation", seed=123,
    image_size=images_size, batch_size=BATCH_SIZE)

base_model = tf.keras.applications.VGG16(input_shape=images_shape,
                                         include_top=False,
                                         weights='imagenet')

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

preprocess_input = tf.keras.applications.vgg16.preprocess_input

for layer in base_model.layers[:-3]:
    layer.trainable = False

inputs = tf.keras.Input(shape=input_size)
model = preprocess_input(inputs)
model = base_model(model)
model = layers.Flatten()(model)
model = layers.Dense(256, activation='relu')(model)
model = layers.Dropout(0.5)(model)
feature = layers.Dense(64, activation='relu', name='feature')(model)

outputs = layers.Dense(10, name='classification',activation='softmax')(feature)
model = tf.keras.Model(inputs, outputs)

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss={'classification': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)},
              metrics=['accuracy'])

model.summary()

import numpy as np
newModel2 = tf.keras.Model(inputs, feature)

rt = np.zeros((1,224, 224, 3))
out = newModel2.predict(rt)
print(out.shape)

model.save('models/')

history = model.fit(train_ds,
                    epochs=EPOCHS,
                    validation_data=val_ds)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']
# triplet_loss = history.history['feature_loss']
# triplet_val_loss = history.history['val_feature_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()), 1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
# plt.plot(triplet_loss, label='Training Triplet Loss')
# plt.plot(triplet_val_loss, label='Validation Triplet Loss')
plt.legend(loc='lower left')
plt.ylabel('Loss')
plt.ylim([0, 1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

model.save('models/')